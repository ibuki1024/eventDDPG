\documentclass{jsarticle}
\usepackage[top=20truemm,bottom=20truemm,left=25truemm,right=25truemm]{geometry}
\usepackage{amsmath,ascmac,url,amsfonts,bm,here,algorithmic,algorithm,amsthm,color}
\usepackage[dvipdfmx]{graphicx}
\newcommand{\argmax}{\mathop{\rm argmax}\limits}
\newcommand{\argmin}{\mathop{\rm argmin}\limits} 
\newcommand{\expect}{\mathbb{E}} 
\newcommand{\trans}[1]{#1^{\top}}
\newcommand{\pdif}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\odif}[2]{\frac{\rm{d}#1}{\rm{d}#2}}
\makeatletter
  \def\@maketitle{
  \newpage\null
  \vskip 2em
    \mbox{}\hfill
    \begin{flushleft}
    \textbf{制御システム論分野研究会資料}
    \end{flushleft}
    \begin{flushright}
    {\lineskip .5em
      \begin{tabular}[t]{c}
        \@date \\
        \@author
      \end{tabular}\par}
        \end{flushright}
  \begin{center}
  \let\footnote\thanks
    {\LARGE \@title \par}
    \vskip 1.5em
  \end{center}
    \vskip 1em
  \par}
\makeatother
\title{\large{\bf{進捗報告 8.20}}}
\author{M2 竹内 維吹}
%\date{Jul. 7, 2020}
\date{\today}
\begin{document}
\maketitle


\section{サンプル値系システムの強化学習}
対象のシステムが
\begin{equation}
	x_{t+1} = f(x_t) + g(x_t)u_t
\end{equation}
と書かれていて,  $\tau$ステップ毎に状態を観測し, 状態フィードバック制御則を変えて次の観測までは同じ入力を加え続ける, サンプル値系における最適制御問題の強化学習を考える. この動機は$\tau$ステップ毎に観測・制御則更新を行うのと, それを毎ステップ行うのとで学習に必要なステップ数が変わるのかを検証するためである.\par
結論から記すと, おそらくはサンプル値系にしたからといって学習時間に大きな変化はないと考える. 
% 1ステップとサンプル値系で同じような制御性能に至るまでに必要なステップをそれぞれ探す

\section{倒立振子による実験}
倒立時の振子の角度を$\theta=0$とし, 加えられる入力が$A=[-10\textrm{N}\cdot\textrm{m},10\textrm{N}\cdot\textrm{m}]$と制限されるような倒立振子を考える.この倒立振子のダイナミクスは, 以下のように与えられる.
\begin{align}
	\theta_{t+1} &= \theta_t+\dot{\theta}_t\delta_t+\frac{3g}{2l}\sin{\theta_t}\delta_t^2+\frac{3}{ml^2}a\delta_t^2 \\
	\dot{\theta}_{t+1} &=  \dot{\theta}_t+\frac{3g}{2l}\sin{\theta_t}\delta_t+\frac{3}{ml^2}a\delta_t
\end{align}
これは式(\ref{true_dynamics})に対応する. 本実験ではこのダイナミクスが既知であるとして$U(s)$を構築し, 状態制約(\ref{constraint})を満たしながら$\pi^{*}$を求めることができるのかを検証する. ただし, $\delta_t$は離散化定数であり$\delta_t=0.005$とする.\par
ただし, これは離散化された状態方程式であるため, CBFによる状態制約の前進不変性をより厳密に議論するために対応する連続時間システムを書き下すと
\begin{equation}
	\odif{}{t}\begin{pmatrix}\theta \\ \dot{\theta}\end{pmatrix} = 
		\begin{pmatrix}\dot{\theta} \\ \frac{3g}{2l}\sin{\theta} + \frac{3}{ml^2}a \end{pmatrix} \label{continuous}
\end{equation}
となる. 


\section{セルフトリガー制御にむけて}
毎ステップ観測の最適なエージェントを初期値として, サンプル間隔$\tau$を変えた時に制御性能を満たしながら"サボる"ことを学習できるのかどうか検証したい. 



\begin{thebibliography}{10}
\bibitem{event}
D. Baumann, J. J. Zhu, G. Martius, and S. Trimpe. “Deep Reinforcement Learning for Event-Triggered Control."  \textit{In Proc. of the 57th IEEE International Conference on Decision and Control}, 2018.
\bibitem{quad}
Li Wang, Evangelos A Theodorou, and Magnus Egerstedt. “Safe learning of quadrotor dynamics using barrier certificates," \textit{In 2018 IEEE International Conference on Robotics and Automation (ICRA)}, pages 2460-2465, 2018
\bibitem{safe}
R. Cheng, G. Orosz, R. M. Murray, and J. W. Burdick.  “End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks," \textit{Thirty-Third AAAI Conference on Artificial Intelligence}, 2019.

\end{thebibliography}

 
\end{document}