\documentclass{jsarticle}
\usepackage[top=20truemm,bottom=20truemm,left=25truemm,right=25truemm]{geometry}
\usepackage{amsmath,ascmac,url,amsfonts,bm,here,algorithmic,algorithm,amsthm,color}
\usepackage[dvipdfmx]{graphicx}
\newcommand{\argmax}{\mathop{\rm argmax}\limits}
\newcommand{\argmin}{\mathop{\rm argmin}\limits} 
\newcommand{\expect}{\mathbb{E}} 
\newcommand{\trans}[1]{#1^{\top}}
\newcommand{\pdif}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\odif}[2]{\frac{\rm{d}#1}{\rm{d}#2}}
\makeatletter
  \def\@maketitle{
  \newpage\null
  \vskip 2em
    \mbox{}\hfill
    \begin{flushleft}
    \textbf{制御システム論分野研究会資料}
    \end{flushleft}
    \begin{flushright}
    {\lineskip .5em
      \begin{tabular}[t]{c}
        \@date \\
        \@author
      \end{tabular}\par}
        \end{flushright}
  \begin{center}
  \let\footnote\thanks
    {\LARGE \@title \par}
    \vskip 1.5em
  \end{center}
    \vskip 1em
  \par}
\makeatother
\title{\large{\bf{進捗報告 9.14}}}
\author{M2 竹内 維吹}
\date{\today}
\begin{document}
\maketitle


\section{前提}
入力アフィン系のセルフトリガー制御を考える. 
\begin{equation}
	\dot{s} = f(s) + g(s)a \label{continuous}
\end{equation}

\subsection{倒立振子による実験}
倒立時の振子の角度を$\theta=0$とし, 加えられる入力が$A=[-10\textrm{N}\cdot\textrm{m},10\textrm{N}\cdot\textrm{m}]$と制限されるような倒立振子を考える.この倒立振子のダイナミクスは, 以下のように与えられる.
\begin{equation}
	\odif{}{t}\begin{pmatrix}\theta \\ \dot{\theta}\end{pmatrix} = 
		\begin{pmatrix}\dot{\theta} \\ \frac{3g}{2l}\sin{\theta} + \frac{3}{ml^2}a \end{pmatrix} \label{pendulum}
\end{equation}

コンピュータで強化学習を行う場合, これを離散化したシステムについて計算を行う必要がある.上記の状態方程式を離散化すると以下のようになる.
\begin{align}
	\theta_{t+1} &= \theta_t+\dot{\theta}_t\delta_t+\frac{3g}{2l}\sin{\theta_t}\delta_t^2+\frac{3}{ml^2}a\delta_t^2 \tag{3a}\\
	\dot{\theta}_{t+1} &=  \dot{\theta}_t+\frac{3g}{2l}\sin{\theta_t}\delta_t+\frac{3}{ml^2}a\delta_t \tag{3b}
\end{align}\label{pend}
\setcounter{equation}{3}
ただし, $\delta_t$は離散化定数である.\par


\section{現状確認}
\subsection{セルフトリガー制御}
図\ref{image}のような制御系を考える.
\begin{figure}[h]
	\centering
 	\includegraphics[width=10cm]{event.png}
 	\caption{制御系} \label{image}
\end{figure}\par
これに対するフィードバック制御を考える. 状態変数$s$を観測してアクチュエータに入力信号を送信することを「インタラクション」と呼ぶと,セルフトリガー制御では, 連続的なインタラクションは行わずに, 次のインタラクションを何秒後に行うかをエージェントが決定する. それを数式上で表すため, エージェントの制御則$\pi(s)$は2つの要素からなるベクトル値関数であるとし, 1つ目の要素はアクチュエータに送信する入力$a$,~2つ目の要素は次にインタラクションを行うまでの時間間隔$\tau~(s:秒)$を表すものとする.また, 次のインタラクションを行う時刻までは1つ前のインタラクションで送信した入力$a$を加え続けるものとする(ZOH制御).

\subsection{目標点の確認}
研究を通しての目標は「安全性を確保しながら, 最適セルフトリガー制御則$\pi^{*}$の強化学習を実現させること」である。ここで
\begin{align}
	\pi^{*} &= \argmax_{\pi}J(\pi) \\
	J(\pi) &= \expect_{s_0\in d_0}[V^{\pi}(s_0)] \\
	% V^{\pi}(s_0) &= \sum_{i=0}^{\infty} \gamma^i\{-s_i^{\top}Qs_i-\pi_1(s_i)^{\top}R\pi_1(s_i)+\lambda \pi_2(s_i)\} \\
	V^{\pi}(s_0) &= \sum_{i=0}^{\infty} \gamma^i C^{\pi}_i \label{value} \\
	C^{\pi}_i &= -\int_{T_i}^{T_{i+1}}(s(t)^{\top}Qs(t)+\tau_ia_i^{\top}Ra_i)\textrm{d}t + \lambda \tau_i, ~T_i = \sum_{l=0}^{i} \tau_l \label{reward}
\end{align}
であり,~$\pi_1, \pi_2$は$\pi$の第1,第2成分である. また, $i$はインタラクションの回数を示し, $a_i, \tau_i$はそれぞれ$i$回目のインタラクションでの方策$\pi$の出力である. \par
さて, 一般的に強化学習では, 1ステップ1ステップの行動の良し悪しを評価して方策を更新していく. 
インタラクションとインタラクションの間の区間を「インターバル」と呼ぶと, 式(\ref{value})より, この問題は各インターバルを1ステップとした強化学習問題であると考えることができる. \par
以下では方策$\pi$を$\theta$でパラメトライズし, $\theta^{*} = \argmax_{\theta}J(\pi_{\theta})$を解くことによって$\pi^{*} = \argmax_{\pi}J(\pi)$を得るものとする. 方策勾配を用いた強化学習では$\nabla_{\theta}J(\pi)$を用いて$\theta^{*}$を求める. その際方策勾配$\nabla_{\theta}J(\pi)$の近似のため, 実環境とのインタラクションによって得られたデータ組$\{s,a,r,s^{\prime}\}$を用いる. 「学習中の安全」という言葉を, 「このデータ組の収集を決められた安全領域$\mathcal{C}$の内部でのみ行うこと」と定義する.\par


\subsection{実現可能性の検証:サンプル値系での実験}
上記の目標を達成する見込みがあるのかを検証するために, サンプル値系での実験を行う. サンプル値系では,セルフトリガー制御と同様に連続的なインタラクションは行わない. セルフトリガー制御との違いは, インタラクションの間隔がエージェントによって状態$s$依存で決定するのではなく, 制御問題の設定として定数$t_{\rm{int}}$で与えられる点である.したがってサンプル値系での制御方策$\pi_{\rm{sample}}$は,アクチュエータに送信する入力信号$a$のみを出力する関数として与える.\par
サンプル値系での実験により, $t_{\rm{int}}=0.001(s)$のサンプル値系での最適方策
\begin{align}
\begin{cases}
	\pi_{\rm{sample},1} = -\argmax_{\pi_{\rm{sample}}} \displaystyle\sum_{i=0}^{\infty} \int_{it_{\rm{int}}}^{(i+1)t_{\rm{int}}}(s(t)^{\top}Qs(t)+a_i^{\top}Ra_i)\textrm{d}t \\
	a_i  = \pi_{\rm{sample}}(s(it_{\rm{int}}))
\end{cases}
\end{align}
を初期方策として,~$t_{\rm{int}}=0.002(s)$のサンプル値系での最適方策
\begin{align}
\begin{cases}
	\pi_{\rm{sample},2} = -\argmax_{\pi_{\rm{sample}}} \displaystyle\sum_{i=0}^{\infty} \int_{it_{\rm{int}}}^{(i+1)t_{\rm{int}}}(s(t)^{\top}Qs(t)+a_i^{\top}Ra_i)\textrm{d}t \\
	a_i  = \pi_{\rm{sample}}(s(it_{\rm{int}}))
\end{cases}
\end{align}
 を学習中の安全性を満たしながら学習できるかを検証する. 

\subsection{セルフトリガー制御への発展}
前節での検証によって, インタラクション間隔を大きくしても安全強化学習を行うことが可能であることを確認できたとする. サンプル値系での制御則は入力信号$a$のみを出力する関数であったので, 入力信号$a$とインタラクション間隔$\tau$の二つの要素を出力する必要があるセルフトリガー制御の初期方策として方策$\pi_{\rm{sample},1}$をそのまま用いることはできない. \par
そこで代替策として,
\begin{align}
\begin{cases}
	\pi_1(s)=\pi_{\rm{sample},1}(s)\\
	\pi_2(s)=0.001
\end{cases}
\end{align}
とする方策$\pi_{\rm{init}}$をセルフトリガー制御の強化学習のための初期方策として用いる.

\section{安全性の定義}
\subsection{インタラクション間隔$\tau$の安全性}
ECBF(後から書きます)


\subsection{入力信号$a$の安全性}
強化学習ではデータの収集に環境とのインタラクションを行う必要がある. DDPGと呼ばれるアルゴリズムは方策オン型の強化学習とよばれ, データの収集方策に学習中の暫定最適方策を用いる. したがって,学習初期の方策では安全性が保証されないことがしばしばある. この課題を解決するために, 制御バリア関数を用いる. \par
関数$h(s)$が以下の条件を満たす時, システム(\ref{continuous})に対する制御バリア関数であるという. 
\begin{equation}
	\sup_{a\in A}\left\{\pdif{h}{s}(f(s)+g(s)a)+K(h(s))\right\} \geq 0 \label{CBF}
\end{equation}
ただし, $K(s)$はクラスK関数である.\par
さて, 2.2節にて登場した安全領域$\mathcal{C}$を
\begin{equation}
	\mathcal{C} = \left\{s \in S~|~h(s)\geq 0\right\}
\end{equation}
として与える. このとき$h(s)$が制御バリア関数であるならば, 状態$s\in C$を初期状態とした時, それ以降の全時刻において, 状態$s$が$s\in C$を満たすようにする入力が存在することを保証する. そのような入力集合は現時刻での状態$s$に依存し,
\begin{equation}
	U(s) = \left\{a \in A~|~\pdif{h}{s}(f(s)+g(s)a)+K(h(s))\geq 0\right\}, \forall s\in C
\end{equation}
としてその集合を与える.\par
学習中の安全性を確保するために, 図2のようにエージェントの出力を$U(s)$の要素に射影するレイヤーを設ける.
\begin{figure}[h]
	\centering
 	\includegraphics[width=10cm]{barrier_certificate.png}
 	\caption{制御系} \label{cbf}
\end{figure}\\
当面は, エージェントの出力$a_{\pi}$に最も近い$U(s)$の元への射影を考える.



\section{今後の方針}
\subsection{シミュレーション環境の構築}
さて, 制御バリア関数による安全性保証は連続システムに対して行われるものである. また, セルフトリガー制御則の第2成分$\tau$の出力ロジックを勾配によって更新できるように, $\tau$を連続値として扱う必要がある. しかし, コンピュータ上でシミュレーションを行うには(\ref{continuous})を時間に関して離散化を行わなくてはならない.\par
そこで, インタラクション間隔$\tau$を整数個に等間隔に分割し, その時間幅を用いて離散化を行う. この時, 離散化幅は$0.001(s)\sim 0.005(s)$になるように分割数を調整する. $\delta_t$を上から抑える理由は離散化誤差を抑えるためである. ($\tau$を)下から抑えるのは,コンピュータ上で$\frac{1}{\infty}=0$となってしまうからで, それを回避するためである.\par
ここで, 式(\ref{reward})のインターバル報酬$C^{\pi}_i$が定積分を用いて表されているため, これをシミュレーション環境で近似する手法を考える. ダイナミクス(\ref{continuous})の離散化幅$\delta_t$の離散近似システムが
\begin{equation}
	s_{t+1} = f_d(s_t, \delta_t) + g_d(s_t, \delta_t)a_t
\end{equation}
と書かれているとする. インタラクション間隔$\tau$を$N$分割した時, $\delta_t = \frac{\tau}{N}$を用いて
\begin{equation}
	C^{\pi}_i \approx -\delta_t\sum_{k=0}^{N}(s_{n_i+k}^{\top}Qs_{n_i+k}+\tau_ia_i^{\top}Ra_i) + \lambda \tau_i
\end{equation}
と近似する. ここで$s_{n_i}$は$i$回目のインタラクションを行った瞬間の状態変数$s(T_i)$と同じ値が代入されるものとする.\par

\subsection{サンプル値系での検証}
2.3節で記述した通り, $t_{\textrm{int}}$が$0.001(s)$の最適方策$\pi_{\textrm{sample},1}$を初期値として,~$0.002(s)$の最適方策$\pi_{\textrm{sample},2}$を安全強化学習できるのか検証する. 本節ではその安全性の確保方法についてもう少し掘り下げて議論する.\par
ECBFを用いることによって, 入力$a$に対するインタラクション間隔$\tau$の限界を与えることができる. その値を$\tau_{\textrm{max}}(a)$と書く. もし$\tau_{\textrm{max}}(a)<0.002$であるなら, CBFを用いて$U(s)$の元$a_{\textrm{safe}}$を選び$\tau_{\textrm{max}}(a_{\textrm{safe}})\geq0.002$となれば, 次のインタラクションまで$a_{\textrm{safe}}$を加え続けても状態変数が$\mathcal{C}$を出ていくことはない.しかし$\forall a \in U(s)$に対して$\tau_{\textrm{max}}(a)<0.002$であるなら, 次のインタラクションを$0.001(s)$後に行うことで, 安全性を確保する必要がある. ~($0.001\leq\tau_{\textrm{max}}(a)<0.002$を仮定)\par
ここまでの議論を整理すると, 「サンプル値系における安全性保証」とは「1. 入力信号の安全性,~2. サンプル間隔の安全性」を保証することになる. これらが行われることを回避する学習方法については今後検討する.


\subsection{セルフトリガー制御の強化学習}
2.4節で記した初期方策$\pi_{\rm{init}}$から, 安全性を保証しながら最適セルフトリガー制御則$\pi^{*}$の学習を試みる. サンプル値系とは異なり, インタラクション間隔はエージェントが決定する. したがって, 方策関数の出力$\pi(s) = \begin{bmatrix}a & \tau\end{bmatrix}$に対して,~$\tau>\tau_{\textrm{max}}(a)$となった時には入力信号$a$を変更する方法と, 通信間隔$\tau$を変更する方法の二種類の選択があり, どちらを行うべきか考える必要がある.


\begin{thebibliography}{10}
\bibitem{ECBF}
G. Yang, C. Belta, and R. Tron. “Self-triggered Control for Safety Critical Systems Using Control Barrier Functions."  \textit{In American Control Conference (ACC) Philadelphia, USA}, 2019.
 
 \end{thebibliography}
\end{document}