\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}はじめに}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}方策勾配を用いた強化学習}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}強化学習の基礎知識}{1}}
\newlabel{purpose_of_rl}{{1}{1}}
\newlabel{Q_func}{{4}{1}}
\citation{DQN}
\citation{DPG}
\citation{DDPG}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}方策反復法}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}状態空間, 行動空間の特性に合わせたアルゴリズム}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}方策勾配による方策関数のパラメータの更新}{2}}
\newlabel{true_pg}{{5}{2}}
\newlabel{d_dis}{{6}{2}}
\newlabel{critic_loss}{{8}{2}}
\newlabel{sample_approximation_for_pg}{{10}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}最適セルフトリガー制御問題に対する強化学習}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}セルフトリガー制御}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces 制御系}}{3}}
\newlabel{image}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}最適セルフトリガー制御}{3}}
\newlabel{optimal_policy}{{11}{3}}
\newlabel{value}{{13}{3}}
\newlabel{reward}{{14}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}数値実験による課題点の抽出}{4}}
\newlabel{pendulum}{{15}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}初期方策}{4}}
\newlabel{pi_init}{{16}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}学習の様子からみる勾配法の進捗の確認}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}勾配法が収束しない原因の検討}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces 学習を通しての$\tau $の変化}}{5}}
\newlabel{average_tau}{{2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces 学習を通しての$\delimiter "026B30D g\delimiter "026B30D $の変化}}{5}}
\newlabel{gradient_log}{{3}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}課題点抽出}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {6}方策勾配の大きさによる, 入力信号$a$と通信間隔$\tau $の学習率の考察}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces actorネットワーク}}{6}}
\newlabel{split_NN}{{4}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {7}2つのパラメータに関する方策勾配の考察}{7}}
\newlabel{recurrence}{{23}{7}}
\newlabel{recurrence_a}{{24}{7}}
\newlabel{recurrence_tau}{{25}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}注意}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}各ステップの報酬のパラメータ勾配}{8}}
\newlabel{reward_tau_gradient}{{29}{8}}
\newlabel{solution}{{30}{8}}
\newlabel{reward_a_gradient}{{33}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {8}修論の見通し}{8}}
\newlabel{tau_gradient}{{36}{9}}
\newlabel{a_gradient}{{37}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {9}イベントトリガーとセルフトリガーの違いについての考察}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}状況整理}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}イベントトリガー制御と, セルフトリガー制御に対する強化学習で違いが生じた原因}{9}}
\bibcite{ECBF}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces 経験データの分布}}{10}}
\newlabel{mini_batch}{{5}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}今後の研究のすすめ方}{10}}
