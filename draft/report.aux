\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}はじめに}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}方策勾配を用いた強化学習}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}強化学習の基礎知識}{1}}
\newlabel{purpose_of_rl}{{1}{1}}
\newlabel{value_function}{{2}{1}}
\newlabel{Q_func}{{4}{1}}
\citation{DQN}
\citation{DPG}
\citation{DDPG}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}方策反復法}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}状態空間, 行動空間の特性に合わせたアルゴリズム}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}方策勾配による方策関数のパラメータの更新}{2}}
\newlabel{true_pg}{{5}{2}}
\newlabel{d_dis}{{6}{2}}
\newlabel{critic_loss}{{8}{2}}
\newlabel{sample_approximation_for_pg}{{10}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}最適セルフトリガー制御問題に対する強化学習}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}セルフトリガー制御}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces 制御系}}{3}}
\newlabel{image}{{1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}最適セルフトリガー制御}{3}}
\newlabel{optimal_policy}{{11}{3}}
\newlabel{value}{{13}{3}}
\newlabel{reward}{{14}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}数値実験による課題点の抽出}{4}}
\newlabel{pendulum}{{15}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}初期方策}{4}}
\newlabel{pi_init}{{16}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}学習によって得られた方策}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces $\pi _{\rm  {RL}}$による制御ログ(角度(rad)と通信間隔(s))}}{4}}
\newlabel{self_trigger_log}{{2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}学習の様子}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces 原点で何も入力を加えない時のcriticと$\tau $のグラフ}}{5}}
\newlabel{tau_gradient_origin}{{3}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces DDPGで得られたcriticによる価値関数$V^{\pi }$の近似}}{5}}
\newlabel{critic_approximation}{{4}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}criticからリアプノフ関数へ}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces 教師あり学習で追加学習したcriticによる価値関数$V^{\pi }$の近似}}{6}}
\newlabel{supervised_critic}{{5}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {6}$Q$関数とcriticネットワークの比較による, 課題点の抽出}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}$Q$関数の近似精度}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces $s=[0,0]$に対する$Q$関数の近似精度}}{7}}
\newlabel{critic_q_const_s}{{6}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}criticの近似と経験データの分散}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}割引分布と探索ノイズ}{7}}
\citation{ECBF}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}探索と利用のジレンマ}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}適応的探索ノイズ}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces セルフトリガー制御における状態変化の上界}}{8}}
\newlabel{s_prime}{{7}{8}}
\bibcite{ECBF}{1}
