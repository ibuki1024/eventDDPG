{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../module/')\n",
    "\n",
    "from keras2.models import Model\n",
    "from keras2.layers import concatenate, Dense, Input, Flatten\n",
    "from keras2.optimizers import Adam\n",
    "import csv\n",
    "from util import *\n",
    "import gym2\n",
    "from rl2.agents import selfDDPGAgent, selfDDPGAgent2\n",
    "from rl2.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GymのPendulum環境を作成\n",
    "env = gym2.make(\"Pendulum-v2\")\n",
    "\n",
    "# 取りうる”打ち手”のアクション数と値の定義\n",
    "nb_actios = 2\n",
    "ACT_ID_TO_VALUE = {0: [-1], 1: [+1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_net(a_shape , s_shape):\n",
    "    action_input = Input(a_shape)\n",
    "    observation_input = Input(shape=(1,)+s_shape)\n",
    "    flattened_observation = Flatten()(observation_input)\n",
    "    x = concatenate([action_input, flattened_observation])\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"linear\")(x)\n",
    "    critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "    return (critic, action_input)\n",
    "\n",
    "def branch_actor(a_shape, s_shape):\n",
    "    action_input = Input(shape=(1,)+s_shape)\n",
    "    x = Flatten()(action_input) # 実質的なinput layer\n",
    "    \n",
    "    x1 = Dense(8, activation=\"relu\")(x)\n",
    "    x1 = Dense(8, activation=\"relu\")(x1)\n",
    "    x1 = Dense(1, activation=\"multiple_tanh\")(x1) # action signal\n",
    "    \n",
    "    x2 = Dense(8, activation=\"relu\")(x)\n",
    "    x2 = Dense(8, activation=\"relu\")(x2)\n",
    "    x2 = Dense(1, activation=\"tau_output\")(x2) # tau\n",
    "    \n",
    "    output = concatenate([x1, x2])\n",
    "    actor = Model(inputs=action_input, outputs=output)\n",
    "    return actor\n",
    "\n",
    "\n",
    "def agent2(a_shape, s_shape):\n",
    "    actor = branch_actor(a_shape, s_shape)\n",
    "    critic,  critic_action_input = critic_net(a_shape, s_shape)\n",
    "    memory = SequentialMemory(limit = 50000, window_length = 1)\n",
    "    agent = selfDDPGAgent2(\n",
    "        a_shape[0],\n",
    "        actor,\n",
    "        critic,\n",
    "        critic_action_input,\n",
    "        memory,\n",
    "        original_noise=True,\n",
    "        action_clipper=[-10., 10.],\n",
    "        tau_clipper=[0.001, 1.],\n",
    "        params_logging=False,\n",
    "        gradient_logging=False,\n",
    "        batch_size=128,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:82: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:525: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:4148: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:182: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:189: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agent compilation\n",
    "l = 1.\n",
    "step = 50000  # num of interval\n",
    "episode_step = step\n",
    "a = agent2((2,), (2,))\n",
    "actor_optimizer, critic_optimizer = Adam(lr=100., clipnorm=1.), Adam(lr=0.001, clipnorm=1.) # actorの方は何でもいい\n",
    "optimizer = [actor_optimizer, critic_optimizer]\n",
    "a.compile(optimizer=optimizer, metrics=[\"mse\"], action_lr=0.0001, tau_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent setup\n",
    "a.actor.load_weights('../saved_agent/learned_self2.h5')\n",
    "a.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.737551039428716\n",
      "45.27157128334659\n",
      "42.777771450231675\n",
      "43.60551256459985\n",
      "42.99812655043393\n",
      "43.23485917918604\n",
      "45.41686950790649\n",
      "44.933579274797985\n",
      "42.25362076854861\n",
      "44.74862063714508\n",
      "43.81502505463005\n",
      "44.05785394121035\n",
      "44.159268703042834\n",
      "44.685518103411226\n",
      "43.943348760205716\n",
      "43.572713516564995\n",
      "43.81539496747632\n",
      "43.35178781707106\n",
      "42.47774923138048\n",
      "44.99886177134553\n",
      "45.071618586671654\n",
      "40.71502536794341\n",
      "44.22457068067606\n",
      "45.30262977788495\n",
      "44.198068019410954\n",
      "45.34307445943531\n",
      "44.81354513069418\n",
      "43.65013819350901\n",
      "45.02088187702669\n",
      "44.30416813680495\n",
      "45.442469563578214\n",
      "45.34861916310616\n",
      "43.883351449673924\n",
      "43.7676358848157\n",
      "44.20525516669809\n",
      "42.95757755004645\n",
      "45.29557256991194\n",
      "43.74564003798045\n",
      "45.072662312581144\n",
      "45.38210267844346\n",
      "45.35840371529238\n",
      "44.16486843307137\n",
      "44.08840756704484\n",
      "43.61965974741432\n",
      "40.921567879420664\n",
      "45.3377207294473\n",
      "43.98022420133566\n",
      "45.34646496246097\n",
      "43.99463961080186\n",
      "45.038139557660806\n"
     ]
    }
   ],
   "source": [
    "# experiment\n",
    "\n",
    "l = 1.\n",
    "\n",
    "view_path = False\n",
    "cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# step wise evaluation\n",
    "\n",
    "step_limit = 50000\n",
    "n_episodes = 50\n",
    "gamma = .99\n",
    "average_reward = 0\n",
    "\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    state_log = []\n",
    "    env.reset()\n",
    "    episode_reward = 0\n",
    "    for steps in range(step_limit):\n",
    "        reward = 0\n",
    "        x = env.state\n",
    "        a_agent, tau = a.forward(x)\n",
    "        state_log.append(x)\n",
    "        action_repetition = int(np.ceil(20 * tau))  # minimum natural number which makes `dt` smaller than 0.005\n",
    "        dt = tau / action_repetition\n",
    "        for p in range(action_repetition):\n",
    "            _,_,r,_ = env.step(np.array([a_agent]), dt, tau)\n",
    "            reward += r\n",
    "        reward *= dt\n",
    "        reward += - 0.01 * a_agent**2 + l * tau # step reward\n",
    "\n",
    "        episode_reward += pow(gamma, steps) * reward\n",
    "    print(episode_reward)\n",
    "    average_reward += episode_reward / n_episodes\n",
    "    state_log = np.array(state_log)\n",
    "    if view_path:\n",
    "        plt.plot(state_log[:,0], state_log[:,1], alpha=0.4, color=cycle[ep])\n",
    "        plt.scatter(state_log[0,0], state_log[0,1], marker='o', color=cycle[ep])\n",
    "        plt.scatter(state_log[-1,0], state_log[-1,1], marker='x', color=cycle[ep])\n",
    "\n",
    "if view_path:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.169006142656144\n"
     ]
    }
   ],
   "source": [
    "print(average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_agentはどうなん？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
