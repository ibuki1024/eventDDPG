{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../module/')\n",
    "\n",
    "from keras2.models import Model\n",
    "from keras2.layers import concatenate, Dense, Input, Flatten\n",
    "from keras2.optimizers import Adam, Optimizer\n",
    "import keras2.backend as K\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import itertools\n",
    "from util import *\n",
    "import gym2\n",
    "from rl2.agents import selfDDPGAgent, selfDDPGAgent2, selfDDPGAgent3\n",
    "from rl2.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym2.make('Linear-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_net(a_shape , s_shape):\n",
    "    action_input = Input(a_shape)\n",
    "    observation_input = Input(shape=(1,)+s_shape)\n",
    "    flattened_observation = Flatten()(observation_input)\n",
    "    x = concatenate([action_input, flattened_observation])\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"linear\")(x)\n",
    "    critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "    return (critic, action_input)\n",
    "\n",
    "def value_net(s_shape):\n",
    "    state_input = Input((1,)+s_shape)\n",
    "    x = Flatten()(state_input)\n",
    "    \n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    x = Dense(1, activation=\"linear\")(x)\n",
    "    value = Model(inputs=state_input, output=x)\n",
    "    return value\n",
    "\n",
    "def branch_actor(a_shape, s_shape):\n",
    "    action_input = Input(shape=(1,)+s_shape)\n",
    "    x = Flatten()(action_input) # 実質的なinput layer\n",
    "    \n",
    "    x1 = Dense(16, activation=\"relu\")(x)\n",
    "    x1 = Dense(16, activation=\"relu\")(x1)\n",
    "    x1 = Dense(1, activation=\"multiple_tanh\")(x1) # action signal\n",
    "    \n",
    "    x2 = Dense(16, activation=\"relu\")(x)\n",
    "    x2 = Dense(16, activation=\"relu\")(x2)\n",
    "    x2 = Dense(1, activation=\"tau_output_large\")(x2) # tau\n",
    "    \n",
    "    output = concatenate([x1, x2])\n",
    "    actor = Model(inputs=action_input, outputs=output)\n",
    "    return actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.5\n",
    "\n",
    "# 1ステップのインタラクション\n",
    "def interaction(state, u, tau, env, ln=0.):\n",
    "    env.reset()\n",
    "    x = np.array(state)\n",
    "    env.set_state(x)\n",
    "    reward = 0\n",
    "    a_agent, tau = u, tau\n",
    "    tau = np.clip(tau, 0.01, 10.)\n",
    "    action_repetition = int(np.ceil(100 * tau))  # minimum natural number which makes `dt` smaller than 0.005\n",
    "    dt = .01\n",
    "    for p in range(action_repetition):\n",
    "        _,r,_,_ = env.step(np.array([a_agent]), dt, tau, ln)\n",
    "        r *= np.exp(- alpha * p * dt)\n",
    "        reward += r\n",
    "    reward *= dt\n",
    "    reward -= beta\n",
    "    state1 = env.state\n",
    "    return reward, state1\n",
    "\n",
    "def value_function(actor, n_episodes=5, init_state = np.array([1,2])):\n",
    "    average_reward = 0\n",
    "    for _ in range(n_episodes):\n",
    "        x = init_state\n",
    "        episode_time = 0\n",
    "        episode_reward = 0\n",
    "        log = []\n",
    "        while True:\n",
    "            a_agent, tau = actor.predict_on_batch(x.reshape(1,1,2))[0]\n",
    "            log.append([x, episode_time])\n",
    "            reward, x = interaction(x, a_agent, tau, env, ln=1.)\n",
    "            episode_reward += np.exp(- alpha * episode_time) * reward\n",
    "            episode_time += tau\n",
    "            if episode_time >= 30.:\n",
    "                log.append([x, episode_time])\n",
    "                break\n",
    "        average_reward += episode_reward\n",
    "    return average_reward / n_episodes\n",
    "\n",
    "def evaluation_function(actor):\n",
    "    s1 = np.linspace(-7, 7, 3)\n",
    "    s2 = np.linspace(-7, 7, 3)\n",
    "    S1, S2 = np.meshgrid(s1, s2)\n",
    "    S1, S2 = S1.flatten(), S2.flatten()\n",
    "    average_value = 0\n",
    "    for i, x in enumerate(zip(S1, S2)):\n",
    "        x = np.array(x)\n",
    "        average_value += value_function(actor, init_state=x)\n",
    "    return average_value / len(S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = branch_actor((2,),(2,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\pi_{prop}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy\n",
    "actor.load_weights('../saved_agent/learned_self_linear_proposed5_actor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evs = []\n",
    "for i in range(50):\n",
    "    evs.append(evaluation_function(actor))\n",
    "    print(f'{int(i * 50 / 100)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\pi_{MB}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy\n",
    "actor.load_weights('../saved_agent/mb_self_extend_noisy_actor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evs = []\n",
    "for i in range(50):\n",
    "    evs.append(evaluation_function(actor))\n",
    "    print(f'{int(i * 50 / 100)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
