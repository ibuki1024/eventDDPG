{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../module/')\n",
    "\n",
    "from keras2.models import Model\n",
    "from keras2.layers import concatenate, Dense, Input, Flatten\n",
    "from keras2.optimizers import Adam\n",
    "import csv\n",
    "import itertools\n",
    "from util import *\n",
    "import gym2\n",
    "from rl2.agents import selfDDPGAgent, selfDDPGAgent2\n",
    "from rl2.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym2.make('Linear-v1')\n",
    "Q = .01 * np.eye(2)\n",
    "R = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_net(a_shape , s_shape):\n",
    "    action_input = Input(a_shape)\n",
    "    observation_input = Input(shape=(1,)+s_shape)\n",
    "    flattened_observation = Flatten()(observation_input)\n",
    "    x = concatenate([action_input, flattened_observation])\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"linear\")(x)\n",
    "    critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "    return (critic, action_input)\n",
    "\n",
    "def branch_actor(a_shape, s_shape):\n",
    "    action_input = Input(shape=(1,)+s_shape)\n",
    "    x = Flatten()(action_input) # 実質的なinput layer\n",
    "    \n",
    "    x1 = Dense(16, activation=\"relu\")(x)\n",
    "    x1 = Dense(16, activation=\"relu\")(x1)\n",
    "    x1 = Dense(1, activation=\"multiple_tanh\")(x1) # action signal\n",
    "    \n",
    "    x2 = Dense(16, activation=\"relu\")(x)\n",
    "    x2 = Dense(16, activation=\"relu\")(x2)\n",
    "    x2 = Dense(1, activation=\"tau_output_large\")(x2) # tau\n",
    "    \n",
    "    output = concatenate([x1, x2])\n",
    "    actor = Model(inputs=action_input, outputs=output)\n",
    "    return actor\n",
    "\n",
    "\n",
    "def agent2(a_shape, s_shape):\n",
    "    actor = branch_actor(a_shape, s_shape)\n",
    "    critic, critic_action_input = critic_net(a_shape, s_shape)\n",
    "    memory = SequentialMemory(limit = 30000, window_length = 1)\n",
    "    agent = selfDDPGAgent2(\n",
    "        a_shape[0],\n",
    "        actor,\n",
    "        critic,\n",
    "        critic_action_input,\n",
    "        memory,\n",
    "        gamma=1.,\n",
    "        mb_noise=True,\n",
    "        action_clipper=[-10., 10.],\n",
    "        tau_clipper=[0.001, 10.],\n",
    "        params_logging=False,\n",
    "        gradient_logging=False,\n",
    "        batch_size=128,\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "def gain(A, B, Q, R, dt=None):\n",
    "    if dt is not None:\n",
    "        Ad, Bd = discretized_system(A, B, dt)  \n",
    "        K = dlqr(Ad,Bd,Q,R)[0]\n",
    "    else:\n",
    "        K = lqr(A,B,Q,R)[0]\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning   \n",
    "l = .1\n",
    "step = 1000000  # num of interval\n",
    "episode_step = step\n",
    "a = agent2((2,), (2,))\n",
    "actor_optimizer, critic_optimizer = Adam(lr=100., clipnorm=1.), Adam(lr=0.001, clipnorm=1.) # actorの方は何でもいい\n",
    "optimizer = [actor_optimizer, critic_optimizer]\n",
    "a.compile(optimizer=optimizer, metrics=[\"mse\"], action_lr=0.0001, tau_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.01\n",
    "alpha = 0.4\n",
    "beta = 0.1\n",
    "time_limit = 20.\n",
    "initial_state = np.array([3.,3.])\n",
    "n_episodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.load_weights('../saved_agent/adaptive_linear3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = 0\n",
    "\n",
    "# episodes from same initial point\n",
    "for episode in range(n_episodes):\n",
    "    env.reset()\n",
    "    env.set_state(initial_state)\n",
    "    state_log = []\n",
    "    action_log = []\n",
    "    communication_log = []\n",
    "    acc_time = 0\n",
    "    \n",
    "    while acc_time < time_limit:\n",
    "        u, tau = a.forward(env.state)\n",
    "        tau = np.round(tau, decimals=2)\n",
    "        tau = np.clip(tau, .01, 10.)\n",
    "        acc_time += tau\n",
    "        action_repetition = int(np.round(tau * 100))  # minimum natural number which makes `dt` smaller than 0.005\n",
    "        for p in range(action_repetition):\n",
    "            if p == 0:\n",
    "                communication_log.append(1)\n",
    "            else:\n",
    "                communication_log.append(0)\n",
    "            action_log.append(u)\n",
    "            state_log.append(env.state)\n",
    "            _,_,_,_ = env.step(np.array([u]), dt, tau)\n",
    "        \n",
    "    state_log = state_log[:2000]\n",
    "    action_log = action_log[:2000]\n",
    "    communication_log = communication_log[:2000]\n",
    "    \n",
    "    assert len(state_log) == 2000, f'acc_time = {acc_time}, steps = {len(state_log)}'\n",
    "    \n",
    "    integral = 0\n",
    "    for k in range(2000): # 20 second (2000 * 0.01)\n",
    "        integral += np.exp(-alpha * dt * k) * (np.dot(np.dot(state_log[k], Q), state_log[k]) + R * action_log[k]**2 + \\\n",
    "                                               beta * communication_log[k])\n",
    "    ev += integral\n",
    "ev = dt * ev / n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13914920016720142"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.actor.load_weights('../saved_agent/mb_self_extend.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = 0\n",
    "\n",
    "# episodes from same initial point\n",
    "for episode in range(n_episodes):\n",
    "    env.reset()\n",
    "    env.set_state(initial_state)\n",
    "    state_log = []\n",
    "    action_log = []\n",
    "    communication_log = []\n",
    "    acc_time = 0\n",
    "    \n",
    "    while acc_time < time_limit:\n",
    "        u, tau = a.forward(env.state)\n",
    "        tau = np.round(tau, decimals=2)\n",
    "        tau = np.clip(tau, .01, 10.)\n",
    "        acc_time += tau\n",
    "        action_repetition = int(np.round(tau * 100))  # minimum natural number which makes `dt` smaller than 0.005\n",
    "        for p in range(action_repetition):\n",
    "            if p == 0:\n",
    "                communication_log.append(1)\n",
    "            else:\n",
    "                communication_log.append(0)\n",
    "            action_log.append(u)\n",
    "            state_log.append(env.state)\n",
    "            _,_,_,_ = env.step(np.array([u]), dt, tau)\n",
    "        \n",
    "    state_log = state_log[:2000]\n",
    "    action_log = action_log[:2000]\n",
    "    communication_log = communication_log[:2000]\n",
    "    \n",
    "    assert len(state_log) == 2000, f'acc_time = {acc_time}, steps = {len(state_log)}'\n",
    "    \n",
    "    integral = 0\n",
    "    for k in range(2000): # 20 second (2000 * 0.01)\n",
    "        integral += np.exp(-alpha * dt * k) * (np.dot(np.dot(state_log[k], Q), state_log[k]) + R * action_log[k]**2 + \\\n",
    "                                               beta * communication_log[k])\n",
    "    ev += integral\n",
    "ev = dt * ev / n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16843965638649283"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = gain(env.A, env.B.reshape(2,1), Q, R, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = 0\n",
    "\n",
    "# episodes from same initial point\n",
    "for episode in range(n_episodes):\n",
    "    env.reset()\n",
    "    env.set_state(initial_state)\n",
    "    state_log = []\n",
    "    action_log = []\n",
    "    communication_log = []\n",
    "    acc_time = 0\n",
    "    \n",
    "    while acc_time < time_limit:\n",
    "        u, tau = np.dot(K, env.state), .01\n",
    "        tau = np.round(tau, decimals=2)\n",
    "        tau = np.clip(tau, .01, 10.)\n",
    "        acc_time += tau\n",
    "        action_repetition = int(np.round(tau * 100))  # minimum natural number which makes `dt` smaller than 0.005\n",
    "        for p in range(action_repetition):\n",
    "            if p == 0:\n",
    "                communication_log.append(1)\n",
    "            else:\n",
    "                communication_log.append(0)\n",
    "            action_log.append(u)\n",
    "            state_log.append(env.state)\n",
    "            _,_,_,_ = env.step(np.array([u]), dt, tau)\n",
    "        \n",
    "    state_log = state_log[:2000]\n",
    "    action_log = action_log[:2000]\n",
    "    communication_log = communication_log[:2000]\n",
    "    \n",
    "    assert len(state_log) == 2000, f'acc_time = {acc_time}, steps = {len(state_log)}'\n",
    "    \n",
    "    integral = 0\n",
    "    for k in range(2000): # 20 second (2000 * 0.01)\n",
    "        integral += np.exp(-alpha * dt * k) * (np.dot(np.dot(state_log[k], Q), state_log[k]) + R * action_log[k]**2 + \\\n",
    "                                               beta * communication_log[k])\n",
    "    ev += integral\n",
    "ev = dt * ev / n_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.315196740205263"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8187307530779818\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(-0.04*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
