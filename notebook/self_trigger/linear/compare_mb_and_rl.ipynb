{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../module/')\n",
    "\n",
    "from keras2.models import Model\n",
    "from keras2.layers import concatenate, Dense, Input, Flatten\n",
    "from keras2.optimizers import Adam\n",
    "import csv\n",
    "from util import *\n",
    "import gym2\n",
    "from rl2.agents import selfDDPGAgent, selfDDPGAgent2\n",
    "from rl2.memory import SequentialMemory\n",
    "from scipy import linalg as slinalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym2.make('Linear-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_net(a_shape , s_shape):\n",
    "    action_input = Input(a_shape)\n",
    "    observation_input = Input(shape=(1,)+s_shape)\n",
    "    flattened_observation = Flatten()(observation_input)\n",
    "    x = concatenate([action_input, flattened_observation])\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"linear\")(x)\n",
    "    critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "    return (critic, action_input)\n",
    "\n",
    "def branch_actor(a_shape, s_shape):\n",
    "    action_input = Input(shape=(1,)+s_shape)\n",
    "    x = Flatten()(action_input) # 実質的なinput layer\n",
    "    \n",
    "    x1 = Dense(16, activation=\"relu\")(x)\n",
    "    x1 = Dense(16, activation=\"relu\")(x1)\n",
    "    x1 = Dense(1, activation=\"multiple_tanh\")(x1) # action signal\n",
    "    \n",
    "    x2 = Dense(16, activation=\"relu\")(x)\n",
    "    x2 = Dense(16, activation=\"relu\")(x2)\n",
    "    x2 = Dense(1, activation=\"tau_output_large\")(x2) # tau\n",
    "    \n",
    "    output = concatenate([x1, x2])\n",
    "    actor = Model(inputs=action_input, outputs=output)\n",
    "    return actor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "beta = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def interaction(state, u, tau, env, ln=0):\n",
    "    env.reset()\n",
    "    x = np.array(state)\n",
    "    env.set_state(x)\n",
    "    reward = 0\n",
    "    a_agent, tau = u, tau\n",
    "    tau = np.clip(tau, 0.01, 10.)\n",
    "    action_repetition = int(np.ceil(100 * tau))  # minimum natural number which makes `dt` smaller than 0.005\n",
    "    dt = .01\n",
    "    for p in range(action_repetition):\n",
    "        _,r,_,_ = env.step(np.array([a_agent]), dt, tau, ln)\n",
    "        r *= np.exp(- alpha * p * dt)\n",
    "        reward += r\n",
    "    reward *= dt\n",
    "    reward -= beta\n",
    "    state1 = env.state\n",
    "    return reward, state1\n",
    "\n",
    "# 評価\n",
    "def evaluation(actor, init_state = np.array([1,2])):\n",
    "    x = init_state\n",
    "    episode_time = 0\n",
    "    episode_reward = 0\n",
    "    log = []\n",
    "    while True:\n",
    "        a_agent, tau = actor.predict_on_batch(x.reshape(1,1,2))[0]\n",
    "        log.append([x, episode_time])\n",
    "        reward, x = interaction(x, a_agent, tau, env, ln=0.)\n",
    "        episode_reward += np.exp(- alpha * episode_time) * reward\n",
    "        episode_time += tau\n",
    "        if episode_time >= 30.:\n",
    "            log.append([x, episode_time])\n",
    "            break\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = branch_actor((2,),(2,))\n",
    "q = branch_actor((2,),(2,))\n",
    "m = branch_actor((2,),(2,))\n",
    "p.load_weights('../saved_agent/learned_self_linear_ideal4_actor.h5')\n",
    "q.load_weights('../saved_agent/learned_self_linear_proposed2_actor.h5')\n",
    "m.load_weights('../saved_agent/mb_self_extend_noisy_actor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\r"
     ]
    }
   ],
   "source": [
    "s1 = np.linspace(-7, 7, 10)\n",
    "s2 = np.linspace(-7, 7, 10)\n",
    "S1, S2 = np.meshgrid(s1, s2)\n",
    "S1, S2 = S1.flatten(), S2.flatten()\n",
    "\n",
    "ev_p = []\n",
    "ev_q = []\n",
    "ev_m = []\n",
    "\n",
    "for i, x in enumerate(zip(S1, S2)):\n",
    "    x = np.array(x)\n",
    "    ev_p.append(evaluation(p, init_state=x))\n",
    "    ev_q.append(evaluation(q, init_state=x))\n",
    "    ev_m.append(evaluation(m, init_state=x))\n",
    "    print(f'{int(i*100/len(S1))}%\\r',end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-49.23492325998381, -5.130225422036564, -78.5858268742794)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ideal learning (small n_steps)\n",
    "np.mean(ev_p), np.max(ev_p), np.min(ev_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7.145947447867522, -4.581974323800083, -15.278393767474245)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# practical\n",
    "np.mean(ev_q), np.max(ev_q), np.min(ev_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-19.605850573533203, -15.386024613028132, -22.95265151974251)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naiive\n",
    "np.mean(ev_m), np.max(ev_m), np.min(ev_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
