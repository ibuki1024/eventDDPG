{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../module/')\n",
    "\n",
    "from keras2.models import Model\n",
    "from keras2.layers import concatenate, Dense, Input, Flatten\n",
    "from keras2.optimizers import Adam\n",
    "import csv\n",
    "from util import *\n",
    "import gym2\n",
    "from rl2.agents import selfDDPGAgent, selfDDPGAgent2\n",
    "from rl2.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym2.make('Linear-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_net(a_shape , s_shape):\n",
    "    action_input = Input(a_shape)\n",
    "    observation_input = Input(shape=(1,)+s_shape)\n",
    "    flattened_observation = Flatten()(observation_input)\n",
    "    x = concatenate([action_input, flattened_observation])\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"linear\")(x)\n",
    "    critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "    return (critic, action_input)\n",
    "\n",
    "def branch_actor(a_shape, s_shape):\n",
    "    action_input = Input(shape=(1,)+s_shape)\n",
    "    x = Flatten()(action_input) # 実質的なinput layer\n",
    "    \n",
    "    x1 = Dense(8, activation=\"relu\")(x)\n",
    "    x1 = Dense(8, activation=\"relu\")(x1)\n",
    "    x1 = Dense(1, activation=\"multiple_tanh\")(x1) # action signal\n",
    "    \n",
    "    x2 = Dense(8, activation=\"relu\")(x)\n",
    "    x2 = Dense(8, activation=\"relu\")(x2)\n",
    "    x2 = Dense(1, activation=\"tau_output\")(x2) # tau\n",
    "    \n",
    "    output = concatenate([x1, x2])\n",
    "    actor = Model(inputs=action_input, outputs=output)\n",
    "    return actor\n",
    "\n",
    "\n",
    "def agent2(a_shape, s_shape):\n",
    "    actor = branch_actor(a_shape, s_shape)\n",
    "    critic, critic_action_input = critic_net(a_shape, s_shape)\n",
    "    memory = SequentialMemory(limit = 50000, window_length = 1)\n",
    "    agent = selfDDPGAgent2(\n",
    "        a_shape[0],\n",
    "        actor,\n",
    "        critic,\n",
    "        critic_action_input,\n",
    "        memory,\n",
    "        mb_noise=False,\n",
    "        coef_u = 2.,\n",
    "        coef_tau = .5,\n",
    "        action_clipper=[-10., 10.],\n",
    "        tau_clipper=[0.001, 1.],\n",
    "        params_logging=False,\n",
    "        gradient_logging=False,\n",
    "        batch_size=128,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:82: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:525: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:4148: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:182: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:189: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#learning   \n",
    "l = .1\n",
    "step = 1000000  # num of interval\n",
    "episode_step = step\n",
    "a = agent2((2,), (2,))\n",
    "actor_optimizer, critic_optimizer = Adam(lr=100., clipnorm=1.), Adam(lr=0.001, clipnorm=1.) # actorの方は何でもいい\n",
    "optimizer = [actor_optimizer, critic_optimizer]\n",
    "a.compile(optimizer=optimizer, metrics=[\"mse\"], action_lr=0.0001, tau_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 118s 12ms/step - reward: -0.0056\n",
      "26 episodes - episode_reward: -2.047 [-6.113, 0.443] - loss: 0.001 - mean_squared_error: 0.003 - mean_q: 0.421\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 290s 29ms/step - reward: -0.0089\n",
      "22 episodes - episode_reward: -4.089 [-5.024, -3.178] - loss: 0.003 - mean_squared_error: 0.005 - mean_q: 1.076\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 358s 36ms/step - reward: -0.0097\n",
      "22 episodes - episode_reward: -4.428 [-4.713, -3.951] - loss: 0.003 - mean_squared_error: 0.005 - mean_q: 1.175\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 245s 24ms/step - reward: -0.0094\n",
      "22 episodes - episode_reward: -4.244 [-4.631, -3.826] - loss: 0.002 - mean_squared_error: 0.004 - mean_q: 1.084\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 202s 20ms/step - reward: -0.0077\n",
      "22 episodes - episode_reward: -3.514 [-4.404, -2.460] - loss: 0.002 - mean_squared_error: 0.004 - mean_q: 1.052\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 127s 13ms/step - reward: -0.0076\n",
      "22 episodes - episode_reward: -3.568 [-8.982, 0.414] - loss: 0.002 - mean_squared_error: 0.004 - mean_q: 1.255\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 253s 25ms/step - reward: -0.0100\n",
      "22 episodes - episode_reward: -4.266 [-9.025, 0.370] - loss: 0.002 - mean_squared_error: 0.003 - mean_q: 1.132\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 268s 27ms/step - reward: -0.0093s - reward:\n",
      "22 episodes - episode_reward: -4.355 [-8.995, -0.190] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.941\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 179s 18ms/step - reward: -0.0146\n",
      "22 episodes - episode_reward: -6.617 [-8.991, 0.335] - loss: 0.001 - mean_squared_error: 0.001 - mean_q: 0.781\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 127s 13ms/step - reward: -0.0102\n",
      "22 episodes - episode_reward: -4.830 [-8.946, 0.308] - loss: 0.000 - mean_squared_error: 0.001 - mean_q: 0.628\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 120s 12ms/step - reward: 1.1866e-04\n",
      "22 episodes - episode_reward: 0.126 [-2.355, 0.419] - loss: 0.000 - mean_squared_error: 0.001 - mean_q: 0.651\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 139s 14ms/step - reward: -0.0026\n",
      "22 episodes - episode_reward: -1.275 [-5.720, 0.278] - loss: 0.000 - mean_squared_error: 0.001 - mean_q: 0.609\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 139s 14ms/step - reward: -0.0011\n",
      "22 episodes - episode_reward: -0.506 [-3.406, 0.372] - loss: 0.000 - mean_squared_error: 0.001 - mean_q: 0.526\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 120s 12ms/step - reward: -6.7206e-04\n",
      "22 episodes - episode_reward: -0.202 [-2.787, 0.379] - loss: 0.000 - mean_squared_error: 0.001 - mean_q: 0.502\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 143s 14ms/step - reward: -1.4204e-04\n",
      "22 episodes - episode_reward: -0.164 [-3.285, 0.422] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.448\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: -2.5766e-05\n",
      "22 episodes - episode_reward: -0.009 [-2.320, 0.372] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.394\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: 2.7698e-04\n",
      "22 episodes - episode_reward: 0.119 [-0.921, 0.299] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.356\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: -0.0022\n",
      "22 episodes - episode_reward: -1.014 [-8.841, 0.397] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.318\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: 7.2838e-04\n",
      "22 episodes - episode_reward: 0.337 [0.156, 0.394] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.285\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      " 1773/10000 [====>.........................] - ETA: 1:57 - reward: 3.1558e-05"
     ]
    }
   ],
   "source": [
    "a.load_weights('../saved_agent/linear_init.h5')\n",
    "out = a.fit(env, l=l, nb_steps=step, visualize=0, verbose=1, nb_max_episode_steps=episode_step, episode_time=5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
