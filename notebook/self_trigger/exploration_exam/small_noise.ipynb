{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../module/')\n",
    "\n",
    "from keras2.models import Model\n",
    "from keras2.layers import concatenate, Dense, Input, Flatten\n",
    "from keras2.optimizers import Adam\n",
    "import csv\n",
    "from util import *\n",
    "import gym2\n",
    "from rl2.agents import selfDDPGAgent, selfDDPGAgent2\n",
    "from rl2.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym2.make('Linear-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_net(a_shape , s_shape):\n",
    "    action_input = Input(a_shape)\n",
    "    observation_input = Input(shape=(1,)+s_shape)\n",
    "    flattened_observation = Flatten()(observation_input)\n",
    "    x = concatenate([action_input, flattened_observation])\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"linear\")(x)\n",
    "    critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "    return (critic, action_input)\n",
    "\n",
    "def branch_actor(a_shape, s_shape):\n",
    "    action_input = Input(shape=(1,)+s_shape)\n",
    "    x = Flatten()(action_input) # 実質的なinput layer\n",
    "    \n",
    "    x1 = Dense(8, activation=\"relu\")(x)\n",
    "    x1 = Dense(8, activation=\"relu\")(x1)\n",
    "    x1 = Dense(1, activation=\"multiple_tanh\")(x1) # action signal\n",
    "    \n",
    "    x2 = Dense(8, activation=\"relu\")(x)\n",
    "    x2 = Dense(8, activation=\"relu\")(x2)\n",
    "    x2 = Dense(1, activation=\"tau_output\")(x2) # tau\n",
    "    \n",
    "    output = concatenate([x1, x2])\n",
    "    actor = Model(inputs=action_input, outputs=output)\n",
    "    return actor\n",
    "\n",
    "\n",
    "def agent2(a_shape, s_shape):\n",
    "    actor = branch_actor(a_shape, s_shape)\n",
    "    critic, critic_action_input = critic_net(a_shape, s_shape)\n",
    "    memory = SequentialMemory(limit = 50000, window_length = 1)\n",
    "    agent = selfDDPGAgent2(\n",
    "        a_shape[0],\n",
    "        actor,\n",
    "        critic,\n",
    "        critic_action_input,\n",
    "        memory,\n",
    "        mb_noise=False,\n",
    "        coef_u=.01,\n",
    "        coef_tau=.001,\n",
    "        action_clipper=[-10., 10.],\n",
    "        tau_clipper=[0.001, 1.],\n",
    "        params_logging=False,\n",
    "        gradient_logging=False,\n",
    "        batch_size=128,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:82: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:525: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:4148: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:182: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/backend/tensorflow_backend.py:189: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From ../../../module/keras2/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#learning   \n",
    "l = .1\n",
    "step = 1000000  # num of interval\n",
    "episode_step = step\n",
    "a = agent2((2,), (2,))\n",
    "actor_optimizer, critic_optimizer = Adam(lr=100., clipnorm=1.), Adam(lr=0.001, clipnorm=1.) # actorの方は何でもいい\n",
    "optimizer = [actor_optimizer, critic_optimizer]\n",
    "a.compile(optimizer=optimizer, metrics=[\"mse\"], action_lr=0.0001, tau_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 91s 9ms/step - reward: -0.0056\n",
      "28 episodes - episode_reward: -1.972 [-2.936, 0.438] - loss: 0.016 - mean_squared_error: 0.032 - mean_q: 0.942\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 93s 9ms/step - reward: -0.0080\n",
      "25 episodes - episode_reward: -3.081 [-3.578, -2.391] - loss: 0.010 - mean_squared_error: 0.020 - mean_q: 1.113\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: -0.0087\n",
      "27 episodes - episode_reward: -3.304 [-4.159, -0.794] - loss: 0.006 - mean_squared_error: 0.012 - mean_q: 1.094\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 97s 10ms/step - reward: -0.0084\n",
      "23 episodes - episode_reward: -3.609 [-4.205, -2.572] - loss: 0.002 - mean_squared_error: 0.005 - mean_q: 0.975\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 103s 10ms/step - reward: -0.0046\n",
      "24 episodes - episode_reward: -2.011 [-3.486, 0.420] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.794\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 95s 10ms/step - reward: -0.0027\n",
      "27 episodes - episode_reward: -0.991 [-5.174, 0.426] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.892\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 96s 10ms/step - reward: -0.0022\n",
      "22 episodes - episode_reward: -1.008 [-4.339, 0.402] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.916\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 107s 11ms/step - reward: -7.4082e-04\n",
      "21 episodes - episode_reward: -0.380 [-5.725, 0.433] - loss: 0.001 - mean_squared_error: 0.001 - mean_q: 0.694\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 98s 10ms/step - reward: 4.7575e-04\n",
      "22 episodes - episode_reward: 0.228 [-0.115, 0.431] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.745\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 98s 10ms/step - reward: 2.4110e-04\n",
      "22 episodes - episode_reward: 0.112 [-0.141, 0.379] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.785\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 109s 11ms/step - reward: 1.7529e-05\n",
      "22 episodes - episode_reward: 0.014 [-0.065, 0.072] - loss: 0.001 - mean_squared_error: 0.001 - mean_q: 0.705\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 109s 11ms/step - reward: 1.1116e-04\n",
      "22 episodes - episode_reward: 0.031 [-0.444, 0.399] - loss: 0.000 - mean_squared_error: 0.001 - mean_q: 0.569\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 2309s 231ms/step - reward: 8.1131e-04\n",
      "22 episodes - episode_reward: 0.385 [0.324, 0.415] - loss: 0.000 - mean_squared_error: 0.001 - mean_q: 0.528\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 11068s 1s/step - reward: -4.4402e-05\n",
      "22 episodes - episode_reward: -0.015 [-1.059, 0.380] - loss: 0.000 - mean_squared_error: 0.001 - mean_q: 0.481\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 47354s 5s/step - reward: 4.4734e-04\n",
      "22 episodes - episode_reward: 0.239 [-0.234, 0.437] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.432\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 140s 14ms/step - reward: 3.4679e-04\n",
      "22 episodes - episode_reward: 0.105 [-0.815, 0.429] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.394\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 125s 13ms/step - reward: 7.6955e-04\n",
      "23 episodes - episode_reward: 0.344 [0.156, 0.406] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.360\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 6.7441e-04\n",
      "21 episodes - episode_reward: 0.313 [0.208, 0.378] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.327\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: 6.1980e-04\n",
      "23 episodes - episode_reward: 0.278 [0.208, 0.321] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.300\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 98s 10ms/step - reward: 4.8618e-04\n",
      "21 episodes - episode_reward: 0.221 [0.016, 0.287] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.278\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: 8.1190e-04\n",
      "23 episodes - episode_reward: 0.362 [0.230, 0.433] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.255\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 95s 10ms/step - reward: 8.8205e-04\n",
      "21 episodes - episode_reward: 0.408 [0.367, 0.431] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.235\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 122s 12ms/step - reward: 9.0012e-04\n",
      "22 episodes - episode_reward: 0.410 [0.109, 0.440] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.218\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 117s 12ms/step - reward: 8.1126e-04\n",
      "22 episodes - episode_reward: 0.374 [0.323, 0.437] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.203\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 133s 13ms/step - reward: 3.8389e-04\n",
      "22 episodes - episode_reward: 0.175 [-3.142, 0.435] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.189\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 129s 13ms/step - reward: 7.3581e-04\n",
      "22 episodes - episode_reward: 0.332 [0.185, 0.413] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.176\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 126s 13ms/step - reward: 7.7962e-04\n",
      "22 episodes - episode_reward: 0.357 [0.317, 0.391] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.165\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: -8.0542e-04\n",
      "22 episodes - episode_reward: -0.092 [-8.838, 0.429] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.154\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 134s 13ms/step - reward: -3.0828e-04\n",
      "33 episodes - episode_reward: -0.274 [-8.861, 0.268] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.140\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: 6.9676e-04\n",
      "35 episodes - episode_reward: 0.194 [-0.058, 0.335] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.139\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 109s 11ms/step - reward: 9.7054e-04\n",
      "28 episodes - episode_reward: 0.347 [0.323, 0.375] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.136\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0041\n",
      "123 episodes - episode_reward: 0.331 [-1.564, 0.426] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.137\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 103s 10ms/step - reward: 0.0073\n",
      "207 episodes - episode_reward: 0.352 [0.234, 0.434] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.177\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 102s 10ms/step - reward: 0.0079\n",
      "235 episodes - episode_reward: 0.334 [0.244, 0.428] - loss: 0.000 - mean_squared_error: 0.001 - mean_q: 0.232\n",
      "\n",
      "Interval 35 (340000 steps performed)\n",
      "10000/10000 [==============================] - 105s 10ms/step - reward: 0.0081\n",
      "250 episodes - episode_reward: 0.323 [-3.299, 0.434] - loss: 0.001 - mean_squared_error: 0.001 - mean_q: 0.267\n",
      "\n",
      "Interval 36 (350000 steps performed)\n",
      "10000/10000 [==============================] - 105s 11ms/step - reward: 0.0103\n",
      "300 episodes - episode_reward: 0.343 [0.129, 0.462] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.297\n",
      "\n",
      "Interval 37 (360000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 105s 11ms/step - reward: 0.0108\n",
      "311 episodes - episode_reward: 0.347 [0.127, 0.448] - loss: 0.001 - mean_squared_error: 0.003 - mean_q: 0.331\n",
      "\n",
      "Interval 38 (370000 steps performed)\n",
      "10000/10000 [==============================] - 107s 11ms/step - reward: 0.0125\n",
      "368 episodes - episode_reward: 0.339 [0.175, 0.450] - loss: 0.002 - mean_squared_error: 0.004 - mean_q: 0.355\n",
      "\n",
      "Interval 39 (380000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0118\n",
      "348 episodes - episode_reward: 0.340 [-2.282, 0.450] - loss: 0.002 - mean_squared_error: 0.004 - mean_q: 0.372\n",
      "\n",
      "Interval 40 (390000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: 0.0131\n",
      "399 episodes - episode_reward: 0.328 [0.117, 0.441] - loss: 0.002 - mean_squared_error: 0.005 - mean_q: 0.391\n",
      "\n",
      "Interval 41 (400000 steps performed)\n",
      "10000/10000 [==============================] - 105s 10ms/step - reward: 0.0124\n",
      "380 episodes - episode_reward: 0.327 [0.021, 0.448] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.399\n",
      "\n",
      "Interval 42 (410000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0119\n",
      "346 episodes - episode_reward: 0.344 [0.149, 0.447] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.405\n",
      "\n",
      "Interval 43 (420000 steps performed)\n",
      "10000/10000 [==============================] - 105s 10ms/step - reward: 0.0110\n",
      "321 episodes - episode_reward: 0.343 [0.197, 0.453] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.402\n",
      "\n",
      "Interval 44 (430000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: 0.0062\n",
      "230 episodes - episode_reward: 0.269 [-3.969, 0.440] - loss: 0.003 - mean_squared_error: 0.005 - mean_q: 0.395\n",
      "\n",
      "Interval 45 (440000 steps performed)\n",
      "10000/10000 [==============================] - 115s 11ms/step - reward: 0.0087\n",
      "328 episodes - episode_reward: 0.264 [-2.606, 0.465] - loss: 0.002 - mean_squared_error: 0.005 - mean_q: 0.378\n",
      "\n",
      "Interval 46 (450000 steps performed)\n",
      "10000/10000 [==============================] - 156s 16ms/step - reward: 0.0114\n",
      "447 episodes - episode_reward: 0.256 [-0.965, 0.434] - loss: 0.002 - mean_squared_error: 0.005 - mean_q: 0.367\n",
      "\n",
      "Interval 47 (460000 steps performed)\n",
      "10000/10000 [==============================] - 167s 17ms/step - reward: 0.0087\n",
      "391 episodes - episode_reward: 0.222 [-0.550, 0.432] - loss: 0.002 - mean_squared_error: 0.005 - mean_q: 0.363\n",
      "\n",
      "Interval 48 (470000 steps performed)\n",
      "10000/10000 [==============================] - 175s 18ms/step - reward: 0.0124\n",
      "439 episodes - episode_reward: 0.282 [-0.335, 0.425] - loss: 0.003 - mean_squared_error: 0.005 - mean_q: 0.369\n",
      "\n",
      "Interval 49 (480000 steps performed)\n",
      "10000/10000 [==============================] - 121s 12ms/step - reward: 0.0130\n",
      "439 episodes - episode_reward: 0.297 [0.045, 0.444] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.380\n",
      "\n",
      "Interval 50 (490000 steps performed)\n",
      "10000/10000 [==============================] - 157s 16ms/step - reward: 0.0138\n",
      "469 episodes - episode_reward: 0.294 [-0.018, 0.455] - loss: 0.003 - mean_squared_error: 0.007 - mean_q: 0.400\n",
      "\n",
      "Interval 51 (500000 steps performed)\n",
      "10000/10000 [==============================] - 140s 14ms/step - reward: 0.0143\n",
      "486 episodes - episode_reward: 0.293 [-0.066, 0.433] - loss: 0.004 - mean_squared_error: 0.007 - mean_q: 0.412\n",
      "\n",
      "Interval 52 (510000 steps performed)\n",
      "10000/10000 [==============================] - 113s 11ms/step - reward: 0.0073\n",
      "244 episodes - episode_reward: 0.299 [-3.263, 0.453] - loss: 0.004 - mean_squared_error: 0.007 - mean_q: 0.415\n",
      "\n",
      "Interval 53 (520000 steps performed)\n",
      "10000/10000 [==============================] - 111s 11ms/step - reward: 0.0033\n",
      "190 episodes - episode_reward: 0.175 [-5.891, 0.418] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.398\n",
      "\n",
      "Interval 54 (530000 steps performed)\n",
      "10000/10000 [==============================] - 130s 13ms/step - reward: 0.0118\n",
      "409 episodes - episode_reward: 0.288 [-0.233, 0.451] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.398\n",
      "\n",
      "Interval 55 (540000 steps performed)\n",
      "10000/10000 [==============================] - 138s 14ms/step - reward: 0.0135\n",
      "459 episodes - episode_reward: 0.294 [-0.151, 0.458] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.396\n",
      "\n",
      "Interval 56 (550000 steps performed)\n",
      "10000/10000 [==============================] - 121s 12ms/step - reward: 0.0140\n",
      "468 episodes - episode_reward: 0.301 [-0.113, 0.458] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.393\n",
      "\n",
      "Interval 57 (560000 steps performed)\n",
      "10000/10000 [==============================] - 133s 13ms/step - reward: 0.0146\n",
      "475 episodes - episode_reward: 0.306 [-0.003, 0.464] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.391\n",
      "\n",
      "Interval 58 (570000 steps performed)\n",
      "10000/10000 [==============================] - 130s 13ms/step - reward: 0.0143\n",
      "497 episodes - episode_reward: 0.288 [-0.032, 0.460] - loss: 0.003 - mean_squared_error: 0.007 - mean_q: 0.400\n",
      "\n",
      "Interval 59 (580000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0141\n",
      "522 episodes - episode_reward: 0.270 [-0.271, 0.459] - loss: 0.004 - mean_squared_error: 0.007 - mean_q: 0.397\n",
      "\n",
      "Interval 60 (590000 steps performed)\n",
      "10000/10000 [==============================] - 145s 14ms/step - reward: 0.0131\n",
      "527 episodes - episode_reward: 0.249 [-0.553, 0.433] - loss: 0.004 - mean_squared_error: 0.007 - mean_q: 0.393\n",
      "\n",
      "Interval 61 (600000 steps performed)\n",
      "10000/10000 [==============================] - 139s 14ms/step - reward: 0.0080\n",
      "290 episodes - episode_reward: 0.274 [-0.253, 0.454] - loss: 0.004 - mean_squared_error: 0.007 - mean_q: 0.392\n",
      "\n",
      "Interval 62 (610000 steps performed)\n",
      "10000/10000 [==============================] - 136s 14ms/step - reward: 0.0042\n",
      "118 episodes - episode_reward: 0.352 [-1.667, 0.459] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.388\n",
      "\n",
      "Interval 63 (620000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: 0.0046\n",
      "132 episodes - episode_reward: 0.350 [-0.119, 0.454] - loss: 0.002 - mean_squared_error: 0.005 - mean_q: 0.380\n",
      "\n",
      "Interval 64 (630000 steps performed)\n",
      "10000/10000 [==============================] - 112s 11ms/step - reward: 0.0025\n",
      "277 episodes - episode_reward: 0.092 [-0.751, 0.419] - loss: 0.002 - mean_squared_error: 0.004 - mean_q: 0.379\n",
      "\n",
      "Interval 65 (640000 steps performed)\n",
      "10000/10000 [==============================] - 124s 12ms/step - reward: -0.0018\n",
      "819 episodes - episode_reward: -0.022 [-0.984, 0.430] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.452\n",
      "\n",
      "Interval 66 (650000 steps performed)\n",
      "10000/10000 [==============================] - 124s 12ms/step - reward: -0.0094\n",
      "864 episodes - episode_reward: -0.109 [-1.143, 0.436] - loss: 0.004 - mean_squared_error: 0.007 - mean_q: 0.441\n",
      "\n",
      "Interval 67 (660000 steps performed)\n",
      "10000/10000 [==============================] - 18727s 2s/step - reward: 0.0058\n",
      "383 episodes - episode_reward: 0.151 [-0.506, 0.433] - loss: 0.004 - mean_squared_error: 0.008 - mean_q: 0.399\n",
      "\n",
      "Interval 68 (670000 steps performed)\n",
      "10000/10000 [==============================] - 156s 16ms/step - reward: 0.0078\n",
      "425 episodes - episode_reward: 0.183 [-0.523, 0.420] - loss: 0.004 - mean_squared_error: 0.009 - mean_q: 0.398\n",
      "\n",
      "Interval 69 (680000 steps performed)\n",
      "10000/10000 [==============================] - 148s 15ms/step - reward: 0.0083\n",
      "393 episodes - episode_reward: 0.212 [-0.377, 0.436] - loss: 0.005 - mean_squared_error: 0.010 - mean_q: 0.402\n",
      "\n",
      "Interval 70 (690000 steps performed)\n",
      "10000/10000 [==============================] - 124s 12ms/step - reward: 0.0060\n",
      "221 episodes - episode_reward: 0.270 [-0.008, 0.436] - loss: 0.004 - mean_squared_error: 0.009 - mean_q: 0.410\n",
      "\n",
      "Interval 71 (700000 steps performed)\n",
      "10000/10000 [==============================] - 117s 12ms/step - reward: 0.0044\n",
      "153 episodes - episode_reward: 0.284 [-0.138, 0.449] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.420\n",
      "\n",
      "Interval 72 (710000 steps performed)\n",
      "10000/10000 [==============================] - 127s 13ms/step - reward: 0.0019\n",
      "175 episodes - episode_reward: 0.113 [-0.398, 0.395] - loss: 0.003 - mean_squared_error: 0.005 - mean_q: 0.428\n",
      "\n",
      "Interval 73 (720000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 115s 11ms/step - reward: -0.0017\n",
      "398 episodes - episode_reward: -0.042 [-0.898, 0.408] - loss: 0.002 - mean_squared_error: 0.005 - mean_q: 0.426\n",
      "\n",
      "Interval 74 (730000 steps performed)\n",
      "10000/10000 [==============================] - 121s 12ms/step - reward: -0.0023\n",
      "709 episodes - episode_reward: -0.033 [-1.175, 0.424] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.434\n",
      "\n",
      "Interval 75 (740000 steps performed)\n",
      "10000/10000 [==============================] - 108s 11ms/step - reward: 0.0029\n",
      "78 episodes - episode_reward: 0.369 [0.175, 0.453] - loss: 0.003 - mean_squared_error: 0.005 - mean_q: 0.399\n",
      "\n",
      "Interval 76 (750000 steps performed)\n",
      "10000/10000 [==============================] - 109s 11ms/step - reward: 8.4765e-04\n",
      "22 episodes - episode_reward: 0.391 [0.354, 0.412] - loss: 0.002 - mean_squared_error: 0.004 - mean_q: 0.381\n",
      "\n",
      "Interval 77 (760000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: 8.2484e-04\n",
      "22 episodes - episode_reward: 0.375 [0.352, 0.396] - loss: 0.002 - mean_squared_error: 0.003 - mean_q: 0.359\n",
      "\n",
      "Interval 78 (770000 steps performed)\n",
      "10000/10000 [==============================] - 110s 11ms/step - reward: 0.0025\n",
      "145 episodes - episode_reward: 0.172 [-0.482, 0.442] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.337\n",
      "\n",
      "Interval 79 (780000 steps performed)\n",
      "10000/10000 [==============================] - 126s 13ms/step - reward: -0.0034\n",
      "853 episodes - episode_reward: -0.039 [-1.113, 0.428] - loss: 0.001 - mean_squared_error: 0.003 - mean_q: 0.384\n",
      "\n",
      "Interval 80 (790000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: -0.0045\n",
      "873 episodes - episode_reward: -0.052 [-1.246, 0.432] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.451\n",
      "\n",
      "Interval 81 (800000 steps performed)\n",
      "10000/10000 [==============================] - 119s 12ms/step - reward: -0.0048\n",
      "886 episodes - episode_reward: -0.055 [-1.161, 0.453] - loss: 0.005 - mean_squared_error: 0.010 - mean_q: 0.463\n",
      "\n",
      "Interval 82 (810000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -0.0034\n",
      "837 episodes - episode_reward: -0.041 [-1.300, 0.436] - loss: 0.008 - mean_squared_error: 0.015 - mean_q: 0.462\n",
      "\n",
      "Interval 83 (820000 steps performed)\n",
      "10000/10000 [==============================] - 106s 11ms/step - reward: 0.0020\n",
      "353 episodes - episode_reward: 0.058 [-0.840, 0.425] - loss: 0.009 - mean_squared_error: 0.017 - mean_q: 0.444\n",
      "\n",
      "Interval 84 (830000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -0.0040\n",
      "772 episodes - episode_reward: -0.052 [-1.260, 0.427] - loss: 0.008 - mean_squared_error: 0.017 - mean_q: 0.457\n",
      "\n",
      "Interval 85 (840000 steps performed)\n",
      "10000/10000 [==============================] - 114s 11ms/step - reward: -0.0037\n",
      "800 episodes - episode_reward: -0.047 [-0.845, 0.439] - loss: 0.008 - mean_squared_error: 0.017 - mean_q: 0.461\n",
      "\n",
      "Interval 86 (850000 steps performed)\n",
      "10000/10000 [==============================] - 115s 12ms/step - reward: -0.0045\n",
      "783 episodes - episode_reward: -0.057 [-1.348, 0.433] - loss: 0.008 - mean_squared_error: 0.017 - mean_q: 0.465\n",
      "\n",
      "Interval 87 (860000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0030\n",
      "294 episodes - episode_reward: 0.103 [-0.876, 0.429] - loss: 0.008 - mean_squared_error: 0.015 - mean_q: 0.460\n",
      "\n",
      "Interval 88 (870000 steps performed)\n",
      "10000/10000 [==============================] - 101s 10ms/step - reward: 0.0011\n",
      "151 episodes - episode_reward: 0.071 [-0.652, 0.397] - loss: 0.006 - mean_squared_error: 0.013 - mean_q: 0.450\n",
      "\n",
      "Interval 89 (880000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: 8.4430e-04\n",
      "183 episodes - episode_reward: 0.046 [-1.132, 0.446] - loss: 0.005 - mean_squared_error: 0.010 - mean_q: 0.444\n",
      "\n",
      "Interval 90 (890000 steps performed)\n",
      "10000/10000 [==============================] - 105s 10ms/step - reward: 9.6655e-04\n",
      "190 episodes - episode_reward: 0.051 [-0.898, 0.406] - loss: 0.004 - mean_squared_error: 0.007 - mean_q: 0.449\n",
      "\n",
      "Interval 91 (900000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: 0.0022\n",
      "227 episodes - episode_reward: 0.097 [-0.681, 0.430] - loss: 0.003 - mean_squared_error: 0.005 - mean_q: 0.456\n",
      "\n",
      "Interval 92 (910000 steps performed)\n",
      "10000/10000 [==============================] - 107s 11ms/step - reward: -0.0063\n",
      "634 episodes - episode_reward: -0.100 [-1.491, 0.436] - loss: 0.003 - mean_squared_error: 0.005 - mean_q: 0.487\n",
      "\n",
      "Interval 93 (920000 steps performed)\n",
      "10000/10000 [==============================] - 104s 10ms/step - reward: 0.0023\n",
      "449 episodes - episode_reward: 0.051 [-1.128, 0.428] - loss: 0.004 - mean_squared_error: 0.007 - mean_q: 0.483\n",
      "\n",
      "Interval 94 (930000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: 8.4657e-04\n",
      "95 episodes - episode_reward: 0.090 [-0.767, 0.412] - loss: 0.003 - mean_squared_error: 0.007 - mean_q: 0.454\n",
      "\n",
      "Interval 95 (940000 steps performed)\n",
      "10000/10000 [==============================] - 100s 10ms/step - reward: 6.3467e-04\n",
      "22 episodes - episode_reward: 0.288 [0.227, 0.328] - loss: 0.003 - mean_squared_error: 0.005 - mean_q: 0.429\n",
      "\n",
      "Interval 96 (950000 steps performed)\n",
      "10000/10000 [==============================] - 95s 10ms/step - reward: 7.0625e-04\n",
      "22 episodes - episode_reward: 0.315 [0.277, 0.352] - loss: 0.002 - mean_squared_error: 0.004 - mean_q: 0.401\n",
      "\n",
      "Interval 97 (960000 steps performed)\n",
      "10000/10000 [==============================] - 96s 10ms/step - reward: 6.8639e-04\n",
      "22 episodes - episode_reward: 0.313 [0.256, 0.362] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.376\n",
      "\n",
      "Interval 98 (970000 steps performed)\n",
      "10000/10000 [==============================] - 99s 10ms/step - reward: 7.5843e-04\n",
      "22 episodes - episode_reward: 0.343 [0.277, 0.406] - loss: 0.000 - mean_squared_error: 0.001 - mean_q: 0.351\n",
      "\n",
      "Interval 99 (980000 steps performed)\n",
      "10000/10000 [==============================] - 97s 10ms/step - reward: 7.1031e-04\n",
      "22 episodes - episode_reward: 0.322 [0.243, 0.441] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.321\n",
      "\n",
      "Interval 100 (990000 steps performed)\n",
      "10000/10000 [==============================] - 95s 10ms/step - reward: 2.3919e-06\n",
      "done, took 11631.287 seconds\n"
     ]
    }
   ],
   "source": [
    "a.load_weights('../saved_agent/linear_init.h5')\n",
    "out = a.fit(env, l=l, nb_steps=step, visualize=0, verbose=1, nb_max_episode_steps=episode_step, episode_time=5.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.training = False\n",
    "\n",
    "env.reset()\n",
    "initial_state = np.array([3., 3.])\n",
    "env.set_state(initial_state)\n",
    "\n",
    "states = [initial_state]\n",
    "detail_states = [initial_state]\n",
    "\n",
    "time_limit = 10\n",
    "time_log = [0.]\n",
    "taus = []\n",
    "acc_time = 0\n",
    "episode_reward = 0\n",
    "i = 0\n",
    "detail_time_log = [0.]\n",
    "\n",
    "action_log = []\n",
    "\n",
    "\n",
    "while True:\n",
    "    reward = 0\n",
    "    x = env.state\n",
    "    a_agent, tau = a.forward(x)\n",
    "    tau = np.clip(tau, .01, 1.)\n",
    "    taus.append(tau)\n",
    "    acc_time += tau\n",
    "    time_log.append(acc_time)\n",
    "    dt = 0.01\n",
    "    action_repetition = int(tau * 100)  # minimum natural number which makes `dt` smaller than 0.005\n",
    "    # print(tau, dt, action_repetition)\n",
    "    for p in range(action_repetition):\n",
    "        action_log.append(a_agent)\n",
    "        _,r,_,_ = env.step(np.array([a_agent]), dt, tau)\n",
    "        reward += r\n",
    "        detail_states.append(env.state)\n",
    "        i += 1\n",
    "        detail_time_log.append(i * dt)\n",
    "    reward *= dt\n",
    "    reward += - 0.01 * a_agent**2 + l * tau\n",
    "    episode_reward += reward\n",
    "    states.append(env.state)\n",
    "    if acc_time > time_limit:\n",
    "        break\n",
    "action_log.append(a.forward(env.state)[0])\n",
    "states = np.array(states)\n",
    "detail_states = np.array(detail_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.subplots_adjust(hspace=0.6)\n",
    "\n",
    "# state\n",
    "plt.subplot(3,1,1)\n",
    "plt.xlabel('control time (s)')\n",
    "plt.ylabel('$\\|s\\|_2$')\n",
    "plt.plot(detail_time_log, np.linalg.norm(detail_states, axis=1))\n",
    "#plt.scatter(time_log, states[:,0], marker='.')\n",
    "\n",
    "# action\n",
    "plt.subplot(3,1,2)\n",
    "plt.xlabel('control time (s)')\n",
    "plt.ylabel('$u$')\n",
    "plt.plot(detail_time_log, action_log, color='red')\n",
    "\n",
    "# communication\n",
    "indices = []\n",
    "for t in np.round(time_log, decimals=2):\n",
    "    if t in np.round(detail_time_log, decimals=2):\n",
    "        indices.append(np.where(t == np.round(detail_time_log, decimals=2))[0][0])\n",
    "com = np.zeros_like(detail_time_log)\n",
    "com[indices] = 1\n",
    "plt.subplot(3,1,3)\n",
    "plt.xlabel('control time (s)')\n",
    "plt.ylabel('interaction (bool)')\n",
    "plt.plot(detail_time_log, com, color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.save_weights('../saved_agent/small_noise0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
