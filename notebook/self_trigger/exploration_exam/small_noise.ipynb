{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../module/')\n",
    "\n",
    "from keras2.models import Model\n",
    "from keras2.layers import concatenate, Dense, Input, Flatten\n",
    "from keras2.optimizers import Adam\n",
    "import csv\n",
    "from util import *\n",
    "import gym2\n",
    "from rl2.agents import selfDDPGAgent, selfDDPGAgent2\n",
    "from rl2.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym2.make('Pendulum-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_net(a_shape , s_shape):\n",
    "    action_input = Input(a_shape)\n",
    "    observation_input = Input(shape=(1,)+s_shape)\n",
    "    flattened_observation = Flatten()(observation_input)\n",
    "    x = concatenate([action_input, flattened_observation])\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dense(1, activation=\"linear\")(x)\n",
    "    critic = Model(inputs=[action_input, observation_input], outputs=x)\n",
    "    return (critic, action_input)\n",
    "\n",
    "def branch_actor(a_shape, s_shape):\n",
    "    action_input = Input(shape=(1,)+s_shape)\n",
    "    x = Flatten()(action_input) # 実質的なinput layer\n",
    "    \n",
    "    x1 = Dense(8, activation=\"relu\")(x)\n",
    "    x1 = Dense(8, activation=\"relu\")(x1)\n",
    "    x1 = Dense(1, activation=\"multiple_tanh\")(x1) # action signal\n",
    "    \n",
    "    x2 = Dense(8, activation=\"relu\")(x)\n",
    "    x2 = Dense(8, activation=\"relu\")(x2)\n",
    "    x2 = Dense(1, activation=\"tau_output\")(x2) # tau\n",
    "    \n",
    "    output = concatenate([x1, x2])\n",
    "    actor = Model(inputs=action_input, outputs=output)\n",
    "    return actor\n",
    "\n",
    "\n",
    "def agent2(a_shape, s_shape):\n",
    "    actor = branch_actor(a_shape, s_shape)\n",
    "    critic, critic_action_input = critic_net(a_shape, s_shape)\n",
    "    memory = SequentialMemory(limit = 50000, window_length = 1)\n",
    "    agent = selfDDPGAgent2(\n",
    "        a_shape[0],\n",
    "        actor,\n",
    "        critic,\n",
    "        critic_action_input,\n",
    "        memory,\n",
    "        mb_noise=False,\n",
    "        coef_u = .01,\n",
    "        coef_tau = .001,\n",
    "        action_clipper=[-10., 10.],\n",
    "        tau_clipper=[0.001, 1.],\n",
    "        params_logging=False,\n",
    "        gradient_logging=False,\n",
    "        batch_size=128,\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/takeuchi_ibuki/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#learning   \n",
    "l = .1\n",
    "step = 1000000  # num of interval\n",
    "episode_step = step\n",
    "a = agent2((2,), (2,))\n",
    "actor_optimizer, critic_optimizer = Adam(lr=100., clipnorm=1.), Adam(lr=0.001, clipnorm=1.) # actorの方は何でもいい\n",
    "optimizer = [actor_optimizer, critic_optimizer]\n",
    "a.compile(optimizer=optimizer, metrics=[\"mse\"], action_lr=0.0001, tau_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "  988/10000 [=>............................] - ETA: 6s - reward: -0.2601WARNING:tensorflow:From /Users/takeuchi_ibuki/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "10000/10000 [==============================] - 62s 6ms/step - reward: -0.0372\n",
      "52 episodes - episode_reward: -7.112 [-24.164, 0.598] - loss: 0.019 - mean_squared_error: 0.039 - mean_q: -0.618\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 71s 7ms/step - reward: -0.0406\n",
      "132 episodes - episode_reward: -3.051 [-8.300, 0.207] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: -0.117\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 75s 7ms/step - reward: -0.0509\n",
      "164 episodes - episode_reward: -3.109 [-8.219, -0.182] - loss: 0.004 - mean_squared_error: 0.008 - mean_q: 0.198\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: -0.0280\n",
      "117 episodes - episode_reward: -2.408 [-7.313, 0.214] - loss: 0.005 - mean_squared_error: 0.010 - mean_q: 0.335\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 84s 8ms/step - reward: -0.0183\n",
      "75 episodes - episode_reward: -2.406 [-6.702, 0.152] - loss: 0.005 - mean_squared_error: 0.011 - mean_q: 0.436\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: -0.0063\n",
      "21 episodes - episode_reward: -3.023 [-10.353, 0.560] - loss: 0.005 - mean_squared_error: 0.010 - mean_q: 0.468\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.0057\n",
      "23 episodes - episode_reward: -2.569 [-5.972, -0.229] - loss: 0.004 - mean_squared_error: 0.009 - mean_q: 0.563\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.0080\n",
      "30 episodes - episode_reward: -2.607 [-6.054, 0.200] - loss: 0.003 - mean_squared_error: 0.006 - mean_q: 0.678\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: -0.0081\n",
      "50 episodes - episode_reward: -1.644 [-5.812, 0.635] - loss: 0.002 - mean_squared_error: 0.004 - mean_q: 0.687\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: -0.0020\n",
      "11 episodes - episode_reward: -1.901 [-4.848, 0.714] - loss: 0.001 - mean_squared_error: 0.003 - mean_q: 0.686\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: -0.0057\n",
      "11 episodes - episode_reward: -4.649 [-8.625, -1.670] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.685\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: -0.0109\n",
      "11 episodes - episode_reward: -9.469 [-12.508, -6.598] - loss: 0.001 - mean_squared_error: 0.002 - mean_q: 0.622\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: -0.0075\n",
      "11 episodes - episode_reward: -7.269 [-17.962, 0.397] - loss: 0.001 - mean_squared_error: 0.001 - mean_q: 0.517\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: -0.0119\n",
      "11 episodes - episode_reward: -11.049 [-30.686, -0.141] - loss: 0.000 - mean_squared_error: 0.001 - mean_q: 0.434\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: -0.0107\n",
      "11 episodes - episode_reward: -9.518 [-26.093, -0.229] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.351\n",
      "\n",
      "Interval 16 (150000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: -0.0054\n",
      "11 episodes - episode_reward: -5.574 [-8.994, -2.852] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.342\n",
      "\n",
      "Interval 17 (160000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: -0.0027\n",
      "11 episodes - episode_reward: -2.125 [-5.254, 0.518] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.367\n",
      "\n",
      "Interval 18 (170000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: -0.0016\n",
      "11 episodes - episode_reward: -1.667 [-4.471, 0.515] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.404\n",
      "\n",
      "Interval 19 (180000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: -0.0017\n",
      "11 episodes - episode_reward: -1.638 [-5.225, 0.221] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.432\n",
      "\n",
      "Interval 20 (190000 steps performed)\n",
      "10000/10000 [==============================] - 89s 9ms/step - reward: -0.0018\n",
      "11 episodes - episode_reward: -1.536 [-4.878, 0.499] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.464\n",
      "\n",
      "Interval 21 (200000 steps performed)\n",
      "10000/10000 [==============================] - 87s 9ms/step - reward: -6.8462e-04\n",
      "11 episodes - episode_reward: -0.733 [-4.676, 0.648] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.498\n",
      "\n",
      "Interval 22 (210000 steps performed)\n",
      "10000/10000 [==============================] - 88s 9ms/step - reward: -0.0019\n",
      "11 episodes - episode_reward: -1.461 [-4.338, 0.594] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.481\n",
      "\n",
      "Interval 23 (220000 steps performed)\n",
      "10000/10000 [==============================] - 87s 9ms/step - reward: -0.0026\n",
      "11 episodes - episode_reward: -2.642 [-5.485, -0.031] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.440\n",
      "\n",
      "Interval 24 (230000 steps performed)\n",
      "10000/10000 [==============================] - 86s 9ms/step - reward: -0.0021\n",
      "11 episodes - episode_reward: -1.738 [-5.979, 0.525] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.394\n",
      "\n",
      "Interval 25 (240000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: -5.2239e-04\n",
      "11 episodes - episode_reward: -0.640 [-3.338, 0.629] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.353\n",
      "\n",
      "Interval 26 (250000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: -0.0029\n",
      "11 episodes - episode_reward: -2.192 [-6.210, 0.358] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.323\n",
      "\n",
      "Interval 27 (260000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: -0.0032\n",
      "11 episodes - episode_reward: -2.965 [-6.354, -1.214] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.298\n",
      "\n",
      "Interval 28 (270000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.0087\n",
      "11 episodes - episode_reward: -7.761 [-10.528, -3.452] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.272\n",
      "\n",
      "Interval 29 (280000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.0087\n",
      "11 episodes - episode_reward: -7.709 [-11.745, -2.123] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.239\n",
      "\n",
      "Interval 30 (290000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.0103\n",
      "11 episodes - episode_reward: -9.441 [-12.657, -2.790] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.202\n",
      "\n",
      "Interval 31 (300000 steps performed)\n",
      "10000/10000 [==============================] - 85s 9ms/step - reward: -0.0120\n",
      "11 episodes - episode_reward: -10.687 [-15.269, -2.823] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.167\n",
      "\n",
      "Interval 32 (310000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.0097\n",
      "11 episodes - episode_reward: -9.334 [-17.844, -2.833] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.141\n",
      "\n",
      "Interval 33 (320000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.0120\n",
      "11 episodes - episode_reward: -10.596 [-18.799, -3.109] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.106\n",
      "\n",
      "Interval 34 (330000 steps performed)\n",
      "10000/10000 [==============================] - 85s 8ms/step - reward: -0.0104\n",
      "11 episodes - episode_reward: -8.874 [-16.255, -3.506] - loss: 0.000 - mean_squared_error: 0.000 - mean_q: 0.062\n",
      "\n",
      "Interval 35 (340000 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8466/10000 [========================>.....] - ETA: 13s - reward: -0.0094done, took 2957.670 seconds\n"
     ]
    }
   ],
   "source": [
    "#a.load_weights('../saved_agent/linear_init.h5')\n",
    "a.actor.load_weights('../saved_agent/sample_02.h5')\n",
    "out = a.fit(env, l=l, nb_steps=step, visualize=0, verbose=1, nb_max_episode_steps=episode_step, episode_time=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
